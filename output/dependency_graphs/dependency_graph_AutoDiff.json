{
  "AutoDiff.A": {
    "id": "AutoDiff.A",
    "component_type": "class",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [
      "test.B"
    ],
    "source_code": "class A(B):\n    pass",
    "start_line": 5,
    "end_line": 6,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor": {
    "id": "AutoDiff.Tensor",
    "component_type": "class",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [
      "AutoDiff._backward",
      "AutoDiff.Tensor.__mul__",
      "AutoDiff.Tensor.__radd__",
      "AutoDiff.Tensor.__add__",
      "AutoDiff.Tensor.backward",
      "AutoDiff.Tensor.__rmul__",
      "AutoDiff.Tensor.__repr__",
      "AutoDiff._op",
      "AutoDiff.build_grad",
      "AutoDiff._prev",
      "AutoDiff.data",
      "AutoDiff.Tensor",
      "AutoDiff.node",
      "AutoDiff.child"
    ],
    "source_code": "class Tensor():\n    '''\n    Converts each Tensor into a class of type Tensor.\n    Operator overloading happens for each operator.\n    exp: \n        x1+x2 => x1 is self\n                 x2 is other\n    After every operation, a new result node is created with operand nodes set to prev. This helps during backpropagation from destination node to source node to calculate gradients.\n    '''\n    def __init__(self,data, _prev=(),_op=''):\n        '''\n        Args:\n            data: Holds the data of the node\n            grad: The gradient Tensor\n            _backward: Function to calculate gradients of prev nodes\n                    => This is called in the one of the prev node operation which is responsible to create the current node.\n            _prev: Keeps track of the previous nodes that generated the current node\n            _op: Holds the operation performed on prev nodes to generate current node\n        '''\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_prev)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self,other):\n        other = other if isinstance(other,Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, (self,other),'+')\n        def _backward():\n            '''This function is executed only when _backward is called explictly or when node._backward() is called. When node._add__ is called, this function don't get executed. It will only be initialized to node._backward = 'function address'\n            '''\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n    \n    def __mul__(self,other):\n        other = other if isinstance(other,Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, (self,other),'*')\n        def _backward():\n            self.grad += out.grad*other.data\n            other.grad += out.grad*self.data\n        out._backward = _backward\n        return out\n    \n\n    '''The __radd__ and __rmul__ represents reverse addition and multiplication\n    Ex: 5+x1, 5 of type int cannot be added with a Tensor class.\n        So in such cases __radd__ is called which converts 5+x1 to x1+5.\n    '''\n    def __radd__(self,other):\n        return self+other\n    \n    def __rmul__(self,other):\n        return self+other\n    \n    def backward(self):\n        '''Perfrom topological sort using DFS.\n        For every directed edge u-v, vertex u comes before v in the ordering.\n        '''\n        visited = set()\n        topo = []\n\n        def build_grad(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_grad(child)\n                topo.append(node)\n\n        '''The gradient of output node is set to 1. \n        Since backward is called only once, it is declared here..\n        '''\n        self.grad = 1             \n        build_grad(self)\n        for node in reversed(topo):\n            node._backward()",
    "start_line": 8,
    "end_line": 87,
    "has_docstring": true,
    "docstring": "\n    Converts each Tensor into a class of type Tensor.\n    Operator overloading happens for each operator.\n    exp: \n        x1+x2 => x1 is self\n                 x2 is other\n    After every operation, a new result node is created with operand nodes set to prev. This helps during backpropagation from destination node to source node to calculate gradients.\n    "
  },
  "AutoDiff.Tensor.__init__": {
    "id": "AutoDiff.Tensor.__init__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [],
    "source_code": "def __init__(self,data, _prev=(),_op=''):\n        '''\n        Args:\n            data: Holds the data of the node\n            grad: The gradient Tensor\n            _backward: Function to calculate gradients of prev nodes\n                    => This is called in the one of the prev node operation which is responsible to create the current node.\n            _prev: Keeps track of the previous nodes that generated the current node\n            _op: Holds the operation performed on prev nodes to generate current node\n        '''\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_prev)\n        self._op = _op",
    "start_line": 17,
    "end_line": 31,
    "has_docstring": true,
    "docstring": "\n        Args:\n            data: Holds the data of the node\n            grad: The gradient Tensor\n            _backward: Function to calculate gradients of prev nodes\n                    => This is called in the one of the prev node operation which is responsible to create the current node.\n            _prev: Keeps track of the previous nodes that generated the current node\n            _op: Holds the operation performed on prev nodes to generate current node\n        "
  },
  "AutoDiff.Tensor.__repr__": {
    "id": "AutoDiff.Tensor.__repr__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [],
    "source_code": "def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"",
    "start_line": 33,
    "end_line": 34,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor.__add__": {
    "id": "AutoDiff.Tensor.__add__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [
      "AutoDiff.Tensor",
      "AutoDiff._backward"
    ],
    "source_code": "def __add__(self,other):\n        other = other if isinstance(other,Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, (self,other),'+')\n        def _backward():\n            '''This function is executed only when _backward is called explictly or when node._backward() is called. When node._add__ is called, this function don't get executed. It will only be initialized to node._backward = 'function address'\n            '''\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out",
    "start_line": 36,
    "end_line": 45,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor.__mul__": {
    "id": "AutoDiff.Tensor.__mul__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [
      "AutoDiff.Tensor",
      "AutoDiff._backward"
    ],
    "source_code": "def __mul__(self,other):\n        other = other if isinstance(other,Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, (self,other),'*')\n        def _backward():\n            self.grad += out.grad*other.data\n            other.grad += out.grad*self.data\n        out._backward = _backward\n        return out",
    "start_line": 47,
    "end_line": 54,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor.__radd__": {
    "id": "AutoDiff.Tensor.__radd__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [],
    "source_code": "def __radd__(self,other):\n        return self+other",
    "start_line": 61,
    "end_line": 62,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor.__rmul__": {
    "id": "AutoDiff.Tensor.__rmul__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [],
    "source_code": "def __rmul__(self,other):\n        return self+other",
    "start_line": 64,
    "end_line": 65,
    "has_docstring": false,
    "docstring": ""
  },
  "AutoDiff.Tensor.backward": {
    "id": "AutoDiff.Tensor.backward",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\AutoDiff.py",
    "relative_path": "AutoDiff.py",
    "depends_on": [
      "AutoDiff.node",
      "AutoDiff.build_grad",
      "AutoDiff.child"
    ],
    "source_code": "def backward(self):\n        '''Perfrom topological sort using DFS.\n        For every directed edge u-v, vertex u comes before v in the ordering.\n        '''\n        visited = set()\n        topo = []\n\n        def build_grad(node):\n            if node not in visited:\n                visited.add(node)\n                for child in node._prev:\n                    build_grad(child)\n                topo.append(node)\n\n        '''The gradient of output node is set to 1. \n        Since backward is called only once, it is declared here..\n        '''\n        self.grad = 1             \n        build_grad(self)\n        for node in reversed(topo):\n            node._backward()",
    "start_line": 67,
    "end_line": 87,
    "has_docstring": true,
    "docstring": "Perfrom topological sort using DFS.\n        For every directed edge u-v, vertex u comes before v in the ordering.\n        "
  },
  "main.x1": {
    "id": "main.x1",
    "component_type": "assignment",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\main.py",
    "relative_path": "main.py",
    "depends_on": [
      "AutoDiff.Tensor"
    ],
    "source_code": "x1 = Tensor(2)",
    "start_line": 4,
    "end_line": 4,
    "has_docstring": false,
    "docstring": ""
  },
  "main.x2": {
    "id": "main.x2",
    "component_type": "assignment",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\main.py",
    "relative_path": "main.py",
    "depends_on": [
      "AutoDiff.Tensor"
    ],
    "source_code": "x2 = Tensor(3)",
    "start_line": 5,
    "end_line": 5,
    "has_docstring": false,
    "docstring": ""
  },
  "main.SimpleFunction": {
    "id": "main.SimpleFunction",
    "component_type": "function",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\main.py",
    "relative_path": "main.py",
    "depends_on": [],
    "source_code": "def SimpleFunction(x1,x2):\n    # Replace this function with your own function declaration.\n    func =  (x1*x2)*(x1+x2)\n    return func",
    "start_line": 7,
    "end_line": 10,
    "has_docstring": false,
    "docstring": ""
  },
  "main.y": {
    "id": "main.y",
    "component_type": "assignment",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\main.py",
    "relative_path": "main.py",
    "depends_on": [
      "main.x1",
      "main.x2",
      "main.SimpleFunction"
    ],
    "source_code": "y = SimpleFunction(x1,x2)",
    "start_line": 12,
    "end_line": 12,
    "has_docstring": false,
    "docstring": ""
  },
  "test.B": {
    "id": "test.B",
    "component_type": "class",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\test.py",
    "relative_path": "test.py",
    "depends_on": [],
    "source_code": "class B:\n    def __init__(self):\n        print(\"hello\")\n        pass",
    "start_line": 1,
    "end_line": 4,
    "has_docstring": false,
    "docstring": ""
  },
  "test.B.__init__": {
    "id": "test.B.__init__",
    "component_type": "method",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\test.py",
    "relative_path": "test.py",
    "depends_on": [],
    "source_code": "def __init__(self):\n        print(\"hello\")\n        pass",
    "start_line": 2,
    "end_line": 4,
    "has_docstring": false,
    "docstring": ""
  },
  "utils._trace": {
    "id": "utils._trace",
    "component_type": "function",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\utils.py",
    "relative_path": "utils.py",
    "depends_on": [
      "utils.edges",
      "utils.child",
      "utils.v",
      "utils.nodes",
      "utils.build"
    ],
    "source_code": "def _trace(root):\n    nodes, edges = set(), set()\n\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))\n                build(child)\n\n    build(root)\n    return nodes, edges",
    "start_line": 5,
    "end_line": 16,
    "has_docstring": false,
    "docstring": ""
  },
  "utils.DrawGraph": {
    "id": "utils.DrawGraph",
    "component_type": "function",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\utils.py",
    "relative_path": "utils.py",
    "depends_on": [
      "utils.n2",
      "utils.n",
      "utils.Digraph",
      "utils.edges",
      "utils.nodes",
      "utils._trace",
      "utils.n1"
    ],
    "source_code": "def DrawGraph(root):\n    dot = Digraph(format='png', graph_attr={'rankdir': 'LR'})  # Left to right\n\n    nodes, edges = _trace(root)\n    for n in nodes:\n        uid = str(id(n))\n        # Add node: show data and grad\n        dot.node(name=uid, label=repr(n), shape='record')\n        if n._op:\n            dot.node(name=uid + n._op, label=n._op)\n            dot.edge(uid + n._op, uid)\n\n    for n1, n2 in edges:\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot",
    "start_line": 18,
    "end_line": 33,
    "has_docstring": false,
    "docstring": ""
  },
  "utils.DrawGraph2": {
    "id": "utils.DrawGraph2",
    "component_type": "function",
    "file_path": "D:\\Projects\\Code2Doc\\knowledge_base\\AutoDiff\\utils.py",
    "relative_path": "utils.py",
    "depends_on": [
      "utils.nx",
      "utils.plt",
      "utils.child",
      "utils.v",
      "utils.build"
    ],
    "source_code": "def DrawGraph2(output_node):\n    G = nx.DiGraph()\n    visited = set()\n\n    def build(v):\n        if v not in visited:\n            visited.add(v)\n            label = repr(v)\n            G.add_node(v, label=label)\n            for child in v._prev:\n                G.add_edge(child, v)\n                build(child)\n\n    build(output_node)\n\n    pos = nx.spring_layout(G)\n    labels = nx.get_node_attributes(G, 'label')\n    nx.draw(G, pos, with_labels=True, labels=labels, node_size=2000, node_color=\"lightblue\", font_size=8)\n    plt.title(\"Computation Graph\")\n    plt.show()",
    "start_line": 35,
    "end_line": 54,
    "has_docstring": false,
    "docstring": ""
  }
}